AFTER ALL MODELS HAVE BEEN LOST...
----------------------------------
fine_tune --name 'cleaned' --name_ext 'frozen_86_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 86
Epoch 24- loss: 0.5143 - val_loss: 0.7059

fine_tune --name 'cleaned' --name_ext 'frozen_86_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 86
Epoch 20 - loss: 0.5325 - val_loss: 0.6905 (0.72348 public score)

fine_tune --name 'cleaned' --name_ext 'frozen_86_dropout_0_4' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.4 --num_freeze_layers 86
Epoch 20 - loss: 0.5123 - val_loss: 0.7017

fine_tune --name 'cleaned' --name_ext 'frozen_86_dropout_0_3' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.3 --num_freeze_layers 86
Epoch 20 - loss: 0.4901 - val_loss: 0.7353 (0.70487 public score)

fine_tune --name 'cleaned' --name_ext 'frozen_86_dropout_0_2' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.2 --num_freeze_layers 86
Epoch 21 - loss: 0.4392 - val_loss: 0.7216 (0.73083 public score)

0.15 VAL DATA
-------------
fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 86
Epoch 21 - loss: 0.5423 - val_loss: 0.7421

fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 86
Epoch 21 - loss: 0.5079 - val_loss: 0.7489

fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 86 --save_best_only False --loss_stop_val 0.46
Epoch 23 - loss: 0.4509 - val_loss: 0.7581 (0.72726 public score)

fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 86 --save_best_only False --loss_stop_val 0.5
Epoch 21 - loss: 0.4831 - val_loss: 0.7221 (0.73270 public score)

fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_4' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.4 --num_freeze_layers 86
Epoch 20 - loss: 0.4929 - val_loss: 0.7371

fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_3' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.3 --num_freeze_layers 86
Epoch 23 - loss: 0.4140 - val_loss: 0.7726

0.15 VAL DATA CROPPED
---------------------
fine_tune --name 'cropped' --name_ext 'frozen_86_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 30 --epochs 30 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 86
Epoch 24 - loss: 0.5539 - val_loss: 0.7662

fine_tune --name 'cropped' --name_ext 'frozen_86_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 30 --epochs 30 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 86
Epoch 26 - loss: 0.4670 - val_loss: 0.7397 (0.68807 public score)

fine_tune --name 'cropped' --name_ext 'frozen_86_dropout_0_5_l2_0_0001' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0.0001 --dropout_p 0.5 --num_freeze_layers 86 --save_best_only False --loss_stop_val 0.49
Epoch 27 - loss: 0.4879 - val_loss: 0.8397 (0.69871 public score)

0.1 VAL DATA CROPPED WITH ADDITIONAL
------------------------------------
fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_2' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.2 --num_freeze_layers 86
Epoch 24 - loss: 0.4228 - val_loss: 0.7288 (0.71206 public score)

fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_3' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.3 --num_freeze_layers 86
Epoch 30 - loss: 0.3376 - val_loss: 0.7494

fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_4' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.4 --num_freeze_layers 86
Epoch 24 - loss: 0.4763 - val_loss: 0.7155 (0.73848 public score)

fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 86
Epoch 24 - loss: 0.4990 - val_loss: 0.7520 (0.69868 public score)

fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 86
Epoch 27- loss: 0.4783 - val_loss: 0.7386

fine_tune --name 'stable' --name_ext 'frozen_96_dropout_0_3' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 30 --epochs 30 --batch_size 256 --l2_reg 0 --dropout_p 0.3 --num_freeze_layers 96
Epoch 27 - loss: 0.3967 - val_loss: 0.7326

fine_tune --name 'stable' --name_ext 'frozen_96_dropout_0_4' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 30 --epochs 30 --batch_size 256 --l2_reg 0 --dropout_p 0.4 --num_freeze_layers 96
Epoch 29- loss: 0.3769 - val_loss: 0.7216

fine_tune --name 'stable' --name_ext 'frozen_96_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 30 --epochs 30 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 96
Epoch 15 - loss: 0.6435 - val_loss: 0.7669

fine_tune --name 'stable' --name_ext 'frozen_96_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 30 --epochs 30 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 96
Epoch 26 - loss: 0.4987 - val_loss: 0.7383
