UNCLEANED DATA 0.1 train val split
----------------------------------
(cca. 0.89 val_loss, 0.90685 public score)
train_top_classifier --lr 0.0001 --epochs 2700 --batch_size 1332 --l2_reg 0 --dropout_p 0.5 --save_model True

(cca 0.83 val_loss, 0.77085 public score and 0.8123 val_loss)
fine_tune --lr 1e-4 --reduce_lr_factor 0.1 --reduce_lr_patience 1 --epochs 100 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 126

(0.8039 val_loss, 0.76231 public score)
fine_tune --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 100 --epochs 100 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 126


CLEANED DATA (NEW VAL DATA) 0.1 train val split (additional data in val set)
----------------------------------------------------------------------------
(0.7627 val_loss, 0.87526 public score)
train_top_classifier --name 'cleaned' --lr 0.0001 --epochs 2700 --batch_size 1332 --l2_reg 0 --dropout_p 0.5 --save_model True

CLEANED DATA (NEW VAL DATA) 0.1 train val split (only train data in val set)
----------------------------------------------------------------------------
(0.8359 val_loss, 0.87120 public score)
train_top_classifier --name 'cleaned' --lr 0.0001 --epochs 1000 --batch_size 1024 --l2_reg 0 --dropout_p 0.5 --save_model True

fine tuning
-----------
fine_tune --name 'cleaned' --name_ext 'frozen_126' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 100 --epochs 100 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 126
Epoch 27 - loss: 0.4802 - val_loss: 0.7614

fine_tune --name 'cleaned' --name_ext 'frozen_116' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 100 --epochs 100 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 116
Epoch 20 - loss: 0.5719 - val_loss: 0.7696

fine_tune --name 'cleaned' --name_ext 'frozen_116_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 100 --epochs 100 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 116
Epoch 32 - loss: 0.4252 - val_loss: 0.7355 (public score 0.74836)

fine_tune --name 'cleaned' --name_ext 'frozen_106_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 100 --epochs 100 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 106
Epoch 24 - loss: 0.5405 - val_loss: 0.7193 (0.73764 public score)

fine_tune --name 'cleaned' --name_ext 'frozen_106_dropout_0_6_all_epochs' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 100 --epochs 40 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 106 --save_best_only False
Epoch 40 - loss: 0.3254 - val_loss: 0.8379 (public score 0.77236)

fine_tune --name 'cleaned' --name_ext 'frozen_96_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 100 --epochs 100 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 96
Epoch 26 - loss: 0.5313 - val_loss: 0.7404 (public score 0.72837)

fine_tune --name 'cleaned' --name_ext 'frozen_86_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 100 --epochs 25 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 86
Epoch 25 - loss: 0.5358 - val_loss: 0.7585 (public score 0.73830)

MORE CLEANED DATA 0.1 train val split (only train data in val set)
------------------------------------------------------------------

fine tuning
-----------
fine_tune --name 'cleaned' --name_ext 'frozen_96_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 100 --epochs 100 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 96
Epoch 18 - loss: 0.5799 - val_loss: 0.7289

fine_tune --name 'cleaned' --name_ext 'frozen_96_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 100 --epochs 100 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 96
Epoch 17 - loss: 0.5828 - val_loss: 0.7368

fine_tune --name 'cleaned' --name_ext 'frozen_96_dropout_0_4' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 100 --epochs 100 --batch_size 256 --l2_reg 0 --dropout_p 0.4 --num_freeze_layers 96
Epoch 19 - loss: 0.5186 - val_loss: 0.7205 (public score 0.71442)

fine_tune --name 'cleaned' --name_ext 'frozen_86_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 100 --epochs 100 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 86
Epoch 19 - loss: 0.5628 - val_loss: 0.7118

fine_tune --name 'cleaned' --name_ext 'frozen_86_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 100 --epochs 100 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 86
Epoch 19 - loss: 0.5457 - val_loss: 0.7051

fine_tune --name 'cleaned' --name_ext 'frozen_86_dropout_0_4' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 100 --epochs 100 --batch_size 256 --l2_reg 0 --dropout_p 0.4 --num_freeze_layers 86
Epoch 20 - loss: 0.4970 - val_loss: 0.7045

fine_tune --name 'cleaned' --name_ext 'frozen_76_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 76
Epoch 17 - loss: 0.6006 - val_loss: 0.7228

fine_tune --name 'cleaned' --name_ext 'frozen_76_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 76
Epoch 18 - loss: 0.5500 - val_loss: 0.7290

fine_tune --name 'cleaned' --name_ext 'frozen_76_dropout_0_4' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.4 --num_freeze_layers 76
Epoch 18 - loss: 0.5244 - val_loss: 0.7416

AFTER ALL MODELS HAVE BEEN LOST...
----------------------------------
fine_tune --name 'cleaned' --name_ext 'frozen_86_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 86
Epoch 24- loss: 0.5143 - val_loss: 0.7059

fine_tune --name 'cleaned' --name_ext 'frozen_86_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 86
Epoch 20 - loss: 0.5325 - val_loss: 0.6905 (0.72348 public score)

fine_tune --name 'cleaned' --name_ext 'frozen_86_dropout_0_4' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.4 --num_freeze_layers 86
Epoch 20 - loss: 0.5123 - val_loss: 0.7017

fine_tune --name 'cleaned' --name_ext 'frozen_86_dropout_0_3' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.3 --num_freeze_layers 86
Epoch 20 - loss: 0.4901 - val_loss: 0.7353 (0.70487 public score)

fine_tune --name 'cleaned' --name_ext 'frozen_86_dropout_0_2' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.2 --num_freeze_layers 86
Epoch 21 - loss: 0.4392 - val_loss: 0.7216 (0.73083 public score)

0.15 VAL DATA
-------------
fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 86
Epoch 21 - loss: 0.5423 - val_loss: 0.7421

fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 86
Epoch 21 - loss: 0.5079 - val_loss: 0.7489

fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 86 --save_best_only False --loss_stop_val 0.46
Epoch 23 - loss: 0.4509 - val_loss: 0.7581 (0.72726 public score)

fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 86 --save_best_only False --loss_stop_val 0.5
Epoch 21 - loss: 0.4831 - val_loss: 0.7221 (0.73270 public score)

fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_4' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.4 --num_freeze_layers 86
Epoch 20 - loss: 0.4929 - val_loss: 0.7371

fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_3' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.3 --num_freeze_layers 86
Epoch 23 - loss: 0.4140 - val_loss: 0.7726

0.15 VAL DATA CROPPED
---------------------
fine_tune --name 'cropped' --name_ext 'frozen_86_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 30 --epochs 30 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 86
Epoch 24 - loss: 0.5539 - val_loss: 0.7662

fine_tune --name 'cropped' --name_ext 'frozen_86_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 30 --epochs 30 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 86
Epoch 26 - loss: 0.4670 - val_loss: 0.7397 (0.68807 public score)

fine_tune --name 'cropped' --name_ext 'frozen_86_dropout_0_5_l2_0_0001' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0.0001 --dropout_p 0.5 --num_freeze_layers 86 --save_best_only False --loss_stop_val 0.49
Epoch 27 - loss: 0.4879 - val_loss: 0.8397 (0.69871 public score)

0.1 VAL DATA CROPPED WITH ADDITIONAL
------------------------------------
a
fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_2' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.2 --num_freeze_layers 86

b
fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_3' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.3 --num_freeze_layers 86

c
fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_4' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.4 --num_freeze_layers 86

fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_5' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.5 --num_freeze_layers 86
Epoch 24 - loss: 0.4990 - val_loss: 0.7520 (0.69868 public score)

d
fine_tune --name 'stable' --name_ext 'frozen_86_dropout_0_6' --lr 1e-5 --reduce_lr_factor 0.1 --reduce_lr_patience 50 --epochs 50 --batch_size 256 --l2_reg 0 --dropout_p 0.6 --num_freeze_layers 86
